{
  "inputs": {
    "train": {
      "use_iter": false,
      "query_per_iter": 50,
      "relation_file": "/projets/iris/PROJETS/WEIR/code/2ndYear/MatchZoo_latest/data/Robust/from_qrels/5_crossValid/rerank_okapi/fold_4/relation_train.txt",
      "batch_per_iter": 5,
      "input_type": "PairGenerator",
      "batch_size": 128,
      "phase": "TRAIN"
    },
    "share": {
      "embed_path": "/projets/iris/PROJETS/WEIR/code/2ndYear/MatchZoo_latest/data/Robust/from_qrels/5_crossValid/rerank_okapi/fold_4/glove_extendStem_300_norm",
      "text1_corpus": "/projets/iris/PROJETS/WEIR/code/2ndYear/MatchZoo_latest/data/Robust/from_qrels/5_crossValid/rerank_okapi/fold_4/corpus_preprocessed.txt",
      "text2_maxlen": 100,
      "text1_maxlen": 10,
      "target_mode": "ranking",
      "text2_corpus": "/projets/iris/PROJETS/WEIR/code/2ndYear/MatchZoo_latest/data/Robust/from_qrels/5_crossValid/rerank_okapi/fold_4/corpus_preprocessed.txt",
      "embed_size": 300,
      "vocab_size": 469195,
      "use_dpool": false,
      "train_embed": true
    },
    "test": {
      "input_type": "ListGenerator",
      "phase": "EVAL",
      "relation_file": "/projets/iris/PROJETS/WEIR/code/2ndYear/MatchZoo_latest/data/Robust/from_qrels/5_crossValid/rerank_okapi/fold_4/relation_test.txt",
      "batch_list": 10
    },
    "valid": {
      "input_type": "ListGenerator",
      "phase": "EVAL",
      "relation_file": "/projets/iris/PROJETS/WEIR/code/2ndYear/MatchZoo_latest/data/Robust/from_qrels/5_crossValid/rerank_okapi/fold_4/relation_valid.txt",
      "batch_list": 10
    },
    "predict": {
      "input_type": "ListGenerator",
      "phase": "PREDICT",
      "relation_file": "/projets/iris/PROJETS/WEIR/code/2ndYear/MatchZoo_latest/data/Robust/from_qrels/5_crossValid/rerank_okapi/fold_4/relation_test.txt",
      "batch_list": 10
    }
  },
  "losses": [
    {
      "object_name": "rank_hinge_loss",
      "object_params": {
        "margin": 0.5
      }
    }
  ],
  "global": {
    "test_weights_iters": 80,
    "learning_rate": 0.001,
    "optimizer": "adam",
    "model_type": "PY",
    "save_weights_iters": 20,
    "display_interval": 10,
    "weights_file": "/projets/iris/PROJETS/WEIR/code/2ndYear/MatchZoo_latest/latest_version/MatchZoo_latest/MatchZoo/examples/trec/weights/my_models/trained_in_qrels_rerank_okapi/weakColl/weights/WC_siameLSTM_Qrels_rerank_okapi_fold_4_WeakCollaboration_Qlstm_60_Dlstm_150_mask0_False_train_embed_True.weights",
    "num_iters": 400
  },
  "model": {
    "setting": {
      "q_lstm_dropout": 0.2,
      "d_lstm_dropout": 0.25,
      "number_q_lstm_units": 60,
      "mask_zero": false,
      "hidden_activation": "relu",
      "number_d_lstm_units": 150,
      "hidden_sizes": [
        50
      ],
      "output_activation": "sigmoid",
      "dropout_rate": 0.25,
      "num_layers": 1
    },
    "model_path": "matchzoo/models/",
    "model_py": "weak_collaboration.WeakCollaboration"
  },
  "net_name": "WeakCollaboration",
  "metrics": [
    "precision@20",
    "ndcg@20",
    "map"
  ],
  "outputs": {
    "predict": {
      "save_format": "TREC",
      "save_path": "/projets/iris/PROJETS/WEIR/code/2ndYear/MatchZoo_latest/latest_version/MatchZoo_latest/MatchZoo/examples/trec/predictions/in_qrels_rerank_okapi/weak_collaborators/predict_WC_siameLSTM_Qrels_rerank_okapi_fold_4_WeakCollaboration_Qlstm_60_Dlstm_150_mask0_False_train_embed_True.txt"
    }
  }
}