{
  "net_name": "WeakCollaboration",
  "global":{
      "model_type": "PY",
      "weights_file": "/local2/users/tbelkace/trained/weights/my_models/conv_weak_coll/CWC_Qrels_rerank_okapi_fold_0_",
      "save_weights_iters": 20,
      "num_iters": 400,
      "display_interval": 10,
      "test_weights_iters": 300,
      "optimizer": "adam",
      "learning_rate": 0.001
  },
  "inputs": {
    "share": {
        "text1_corpus": "/local2/users/tbelkace/MZ_data/Robust/from_qrels/5_crossValid/rerank_okapi/fold_0/corpus_preprocessed.txt",
        "text2_corpus": "/local2/users/tbelkace/MZ_data/Robust/from_qrels/5_crossValid/rerank_okapi/fold_0/corpus_preprocessed.txt",
        "use_dpool": false,
        "embed_size": 300,
        "embed_path": "/local2/users/tbelkace/MZ_data/Robust/from_qrels/5_crossValid/rerank_okapi/fold_0/glove_extendStem_300_norm",
        "vocab_size": 468040,
        "train_embed": false,
        "target_mode": "ranking",
        "text1_maxlen": 10,
        "text2_maxlen": 100
    },
    "train": {
        "input_type": "PairGenerator",
        "phase": "TRAIN",
        "use_iter": false,
        "query_per_iter": 50,
        "batch_per_iter": 5,
        "batch_size": 64,
        "relation_file": "/local2/users/tbelkace/MZ_data/Robust/from_qrels/5_crossValid/rerank_okapi/fold_0/relation_train.txt"
    },
    "valid": {
        "input_type": "ListGenerator",
        "phase": "EVAL",
        "batch_list": 10,
        "relation_file": "/local2/users/tbelkace/MZ_data/Robust/from_qrels/5_crossValid/rerank_okapi/fold_0/relation_valid.txt"
    },
    "test": {
        "input_type": "ListGenerator",
        "phase": "EVAL",
        "batch_list": 10,
        "relation_file": "/local2/users/tbelkace/MZ_data/Robust/from_qrels/5_crossValid/rerank_okapi/fold_0/relation_test.txt"
    },
    "predict": {
        "input_type": "ListGenerator",
        "phase": "PREDICT",
        "batch_list": 10,
        "relation_file": "/local2/users/tbelkace/MZ_data/Robust/from_qrels/5_crossValid/rerank_okapi/fold_0/relation_test.txt"
    }
  },
  "outputs": {
    "predict": {
      "save_format": "TREC",
      "save_path": "/local2/users/tbelkace/trained/predictions/CWC_Qrels_rerank_okapi_fold_0_"
    }
  },
  "model": {
    "model_path": "matchzoo/models/",
    "model_py": "conv_weak_collaboration.ConvWeakCollaboration",
    "setting": {
        "number_q_lstm_units": 100,
        "number_d_lstm_units": 100,
        "num_layers": 1,
        "hidden_sizes": [32],
        "hidden_activation": "relu",
        "output_activation": "sigmoid",
        "q_lstm_dropout": 0.3,
        "d_lstm_dropout": 0.3,
        "dropout_rate": 0.15,
        "conv_dropout": 0.1,
        "mask_zero": false,
        "filters": 300,
        "kernel_size": 2,
        "conv_activation": "relu",
        "pool_size": 3
    }
  },
  "losses": [
    {
       "object_name": "rank_hinge_loss" ,
       "object_params": {
            "margin": 0.5
       }
    }
  ],
  "metrics": [ "precision@20",
    "ndcg@20",
    "map" ]
}

